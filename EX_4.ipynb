{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c87267f",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a507426",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ddf6cf",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "# urls = [\n",
    "#     \"https://openreview.net/pdf?id=5atraF1tbg\",\n",
    "#     \"https://openreview.net/pdf?id=YaEozn3y0G\",\n",
    "#     \"https://openreview.net/pdf?id=P6NcRPb13w\",\n",
    "# ]\n",
    "\n",
    "papers = [\n",
    "    \"privacy.pdf\",\n",
    "    \"ml_topo.pdf\",\n",
    "    \"ml4.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8cb959",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: privacy.pdf\n",
      "Getting tools for paper: ml_topo.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 209 0 (offset 0)\n",
      "Ignoring wrong pointing object 355 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: ml4.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84fbc3af",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "948603fe",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538c1d10",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51b25d91",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcaa8d35",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the Topology used in Machine Learning, and then tell me about privacy\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_ml_topo with args: {\"query\": \"Topology in Machine Learning\"}\n",
      "=== Function Output ===\n",
      "Topology in machine learning is utilized in the proposed framework for enhanced fine-mapping in whole-genome bacterial studies. The framework incorporates genomic context derived from graph-structured data, specifically based on the compacted de Bruijn graph for an assembled pangenome. This approach aims to improve control for population structure and enhance interpretability by leveraging unique mappings between the encoded feature space and sequential representations that tag specific genomic loci.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_privacy with args: {\"query\": \"Privacy\"}\n",
      "=== Function Output ===\n",
      "The concept of privacy in the context provided relates to the dependency between the model's output and the data it was trained on, particularly in the context of membership inference attacks. This dependency is influenced by the differential privacy properties of the model, which can help in bounding the extent of information leakage regarding the training data.\n",
      "=== LLM Response ===\n",
      "Topology in machine learning is utilized in the proposed framework for enhanced fine-mapping in whole-genome bacterial studies. The framework incorporates genomic context derived from graph-structured data, specifically based on the compacted de Bruijn graph for an assembled pangenome. This approach aims to improve control for population structure and enhance interpretability by leveraging unique mappings between the encoded feature space and sequential representations that tag specific genomic loci.\n",
      "\n",
      "The concept of privacy in the context provided relates to the dependency between the model's output and the data it was trained on, particularly in the context of membership inference attacks. This dependency is influenced by the differential privacy properties of the model, which can help in bounding the extent of information leakage regarding the training data.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the Topology used in Machine Learning, \"\n",
    "    \"and then tell me about privacy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32226f5d",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Adjusting Machine Learning and Topology in Machine Learning\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_ml4 with args: {\"input\": \"Adjusting Machine Learning\"}\n",
      "=== Function Output ===\n",
      "Adjusting Machine Learning involves modifying existing ML decision-makers to produce fair algorithmic decisions that meet criteria such as equal counterfactual opportunity (eco) and counterfactual fairness (cf). These adjustments aim to correct biases in historical decisions while maintaining fidelity to the original data, ensuring fairness without the need for retraining the ML model. By incorporating causal models and criteria, the adjustments strive to provide fair decisions while preserving accuracy and addressing issues of bias and discrimination in decision-making processes.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_ml_topo with args: {\"input\": \"Topology in Machine Learning\"}\n",
      "=== Function Output ===\n",
      "Topology in Machine Learning is utilized to capture spatial context and structural relationships within genomic data, specifically in the context of addressing issues related to non-uniqueness in learned mappings of statistical models. By incorporating topological information from the compacted de Bruijn graph (cDBG) representation of the bacterial pangenome, a feature selection framework selects subgraphs that encode the topological structure of the genome. This approach aims to preserve biologically relevant interactions, aid in feature selection, reduce multicollinearity, and improve interpretability in the analysis of bacterial genome-wide association studies. The use of topology in machine learning facilitates the identification of resistance mechanisms that involve multiple interacting variants, enhancing the focus and accuracy of genetic marker sequence identification for phenotype prediction.\n",
      "=== LLM Response ===\n",
      "Here are the summaries of \"Adjusting Machine Learning\" and \"Topology in Machine Learning\":\n",
      "\n",
      "1. **Adjusting Machine Learning**:\n",
      "   Adjusting Machine Learning involves modifying existing ML decision-makers to produce fair algorithmic decisions that meet criteria such as equal counterfactual opportunity (eco) and counterfactual fairness (cf). These adjustments aim to correct biases in historical decisions while maintaining fidelity to the original data, ensuring fairness without the need for retraining the ML model. By incorporating causal models and criteria, the adjustments strive to provide fair decisions while preserving accuracy and addressing issues of bias and discrimination in decision-making processes.\n",
      "\n",
      "2. **Topology in Machine Learning**:\n",
      "   Topology in Machine Learning is utilized to capture spatial context and structural relationships within genomic data, specifically in the context of addressing issues related to non-uniqueness in learned mappings of statistical models. By incorporating topological information from the compacted de Bruijn graph (cDBG) representation of the bacterial pangenome, a feature selection framework selects subgraphs that encode the topological structure of the genome. This approach aims to preserve biologically relevant interactions, aid in feature selection, reduce multicollinearity, and improve interpretability in the analysis of bacterial genome-wide association studies. The use of topology in machine learning facilitates the identification of resistance mechanisms that involve multiple interacting variants, enhancing the focus and accuracy of genetic marker sequence identification for phenotype prediction.\n",
      "assistant: Here are the summaries of \"Adjusting Machine Learning\" and \"Topology in Machine Learning\":\n",
      "\n",
      "1. **Adjusting Machine Learning**:\n",
      "   Adjusting Machine Learning involves modifying existing ML decision-makers to produce fair algorithmic decisions that meet criteria such as equal counterfactual opportunity (eco) and counterfactual fairness (cf). These adjustments aim to correct biases in historical decisions while maintaining fidelity to the original data, ensuring fairness without the need for retraining the ML model. By incorporating causal models and criteria, the adjustments strive to provide fair decisions while preserving accuracy and addressing issues of bias and discrimination in decision-making processes.\n",
      "\n",
      "2. **Topology in Machine Learning**:\n",
      "   Topology in Machine Learning is utilized to capture spatial context and structural relationships within genomic data, specifically in the context of addressing issues related to non-uniqueness in learned mappings of statistical models. By incorporating topological information from the compacted de Bruijn graph (cDBG) representation of the bacterial pangenome, a feature selection framework selects subgraphs that encode the topological structure of the genome. This approach aims to preserve biologically relevant interactions, aid in feature selection, reduce multicollinearity, and improve interpretability in the analysis of bacterial genome-wide association studies. The use of topology in machine learning facilitates the identification of resistance mechanisms that involve multiple interacting variants, enhancing the focus and accuracy of genetic marker sequence identification for phenotype prediction.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Adjusting Machine Learning and Topology in Machine Learning\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a088430",
   "metadata": {
    "height": 472
   },
   "outputs": [],
   "source": [
    "# urls = [\n",
    "#     \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "#     \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "#     \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "#     \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "#     \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "#     \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "#     \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "#     \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "#     \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "#     \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "#     \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "# ]\n",
    "\n",
    "papers = [\n",
    "    \"ml2.pdf\",\n",
    "    \"ml3.pdf\",\n",
    "    \"ml4.pdf\",\n",
    "    \"ML_blockchain.pdf\",\n",
    "    \"ML_packages.pdf\",\n",
    "    \"ML_pipelines.pdf\",\n",
    "    \"ml_topo.pdf\",\n",
    "    \"privacy.pdf\",\n",
    "    \"MachineLearning.pdf\",\n",
    "    \"memorization.pdf\",\n",
    "    \"ai.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49584106",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: ml2.pdf\n",
      "Getting tools for paper: ml3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 209 0 (offset 0)\n",
      "Ignoring wrong pointing object 355 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: ml4.pdf\n",
      "Getting tools for paper: ML_blockchain.pdf\n",
      "Getting tools for paper: ML_packages.pdf\n",
      "Getting tools for paper: ML_pipelines.pdf\n",
      "Getting tools for paper: ml_topo.pdf\n",
      "Getting tools for paper: privacy.pdf\n",
      "Getting tools for paper: MachineLearning.pdf\n",
      "Getting tools for paper: memorization.pdf\n",
      "Getting tools for paper: ai.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58dbb2ba",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bae04681",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b843052b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5db70806",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the formal definition of memorisation used in Machine Learning and Regularization\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d918b661",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to ml4', name='summary_tool_ml4', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de84e1db",
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f4bf8ac",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the formal definition of memorisation usedin Machine Learning and Regularization\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_memorization with args: {\"input\": \"Formal definition of memorisation in Machine Learning\"}\n",
      "=== Function Output ===\n",
      "The formal definition of memorisation in Machine Learning is the impact a particular sample has on its own prediction, known as self-influence. It is quantified as the difference in performance on the sample when it is included in the training dataset compared to when it is not included in the dataset. This definition is context agnostic and can be applied to various learning settings by selecting a suitable performance metric.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_MachineLearning with args: {\"input\": \"Formal definition of Regularization\"}\n",
      "=== Function Output ===\n",
      "Regularization is a method in machine learning that involves adding a penalty term to the model's loss function. This penalty term is designed to discourage the model from overfitting the training data by either constraining the model's complexity or by penalizing large coefficient values. By doing so, regularization helps the model generalize better to unseen data.\n",
      "=== LLM Response ===\n",
      "The formal definition of memorisation in Machine Learning is the impact a particular sample has on its own prediction, known as self-influence. It is quantified as the difference in performance on the sample when it is included in the training dataset compared to when it is not included in the dataset. This definition is context agnostic and can be applied to various learning settings by selecting a suitable performance metric.\n",
      "\n",
      "Regularization is a method in machine learning that involves adding a penalty term to the model's loss function. This penalty term is designed to discourage the model from overfitting the training data by either constraining the model's complexity or by penalizing large coefficient values. By doing so, regularization helps the model generalize better to unseen data.\n",
      "assistant: The formal definition of memorisation in Machine Learning is the impact a particular sample has on its own prediction, known as self-influence. It is quantified as the difference in performance on the sample when it is included in the training dataset compared to when it is not included in the dataset. This definition is context agnostic and can be applied to various learning settings by selecting a suitable performance metric.\n",
      "\n",
      "Regularization is a method in machine learning that involves adding a penalty term to the model's loss function. This penalty term is designed to discourage the model from overfitting the training data by either constraining the model's complexity or by penalizing large coefficient values. By doing so, regularization helps the model generalize better to unseen data.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the formal definition of memorisation used\" \n",
    "    \"in Machine Learning and Regularization\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a972739b",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Logistic Regression in Machine LearningAnalyze the abstract in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_MachineLearning with args: {\"input\": \"Logistic Regression in Machine Learning\"}\n",
      "=== Function Output ===\n",
      "Logistic regression in machine learning is commonly used to predict paper acceptance in academic conferences based on factors such as average reviewer scores, author reputation, institutional bias, and visibility on platforms like arXiv. By analyzing these variables, logistic regression models provide insights into the decision-making process of paper reviews, identifying biases, correlations with impact, and trends over time. The models aim to understand how different factors influence the acceptance decisions, allowing for a more informed and data-driven approach to the review process.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_ml4 with args: {\"input\": \"Logistic Regression in Machine Learning\"}\n",
      "=== Function Output ===\n",
      "Logistic regression is a widely used method in machine learning for binary classification tasks. It estimates the probability of a binary outcome based on one or more independent variables by fitting a logistic function to the data. This model is valued for its simplicity, interpretability, and effectiveness in various applications such as predicting customer churn, spam detection, and medical diagnosis. Additionally, logistic regression can be utilized to predict decisions accurately based on historical data, where the decision maker draws decisions from a Bernoulli distribution. Adjusting the model's parameters during training minimizes the error between predicted probabilities and actual outcomes, making logistic regression a valuable tool in algorithmic decision-making processes.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_ml2 with args: {\"input\": \"Logistic Regression in Machine Learning\"}\n",
      "=== Function Output ===\n",
      "Logistic Regression is a widely used algorithm in machine learning for binary classification tasks. It models the probability of a binary outcome by fitting a logistic function to the data. This function maps input features to a probability score between 0 and 1, allowing predictions to be made based on whether the probability exceeds a specified threshold, typically set at 0.5. The algorithm is valued for its simplicity, interpretability, and efficiency, making it a popular choice for predictive modeling when the relationship between input features and the target variable is assumed to be linear.\n",
      "=== LLM Response ===\n",
      "Here are the analyses of the abstracts from the papers on Logistic Regression in Machine Learning:\n",
      "\n",
      "1. Paper 1: Logistic regression in machine learning is commonly used to predict paper acceptance in academic conferences based on factors such as average reviewer scores, author reputation, institutional bias, and visibility on platforms like arXiv. The models aim to understand how different factors influence the acceptance decisions, allowing for a more informed and data-driven approach to the review process.\n",
      "\n",
      "2. Paper 2: Logistic regression is a widely used method in machine learning for binary classification tasks. It estimates the probability of a binary outcome based on independent variables by fitting a logistic function to the data. The model is valued for its simplicity, interpretability, and effectiveness in various applications such as predicting customer churn, spam detection, and medical diagnosis.\n",
      "\n",
      "3. Paper 3: Logistic Regression is a widely used algorithm in machine learning for binary classification tasks. It models the probability of a binary outcome by fitting a logistic function to the data. The algorithm is valued for its simplicity, interpretability, and efficiency, making it a popular choice for predictive modeling when the relationship between input features and the target variable is assumed to be linear.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Logistic Regression in Machine Learning\"\n",
    "    \"Analyze the abstract in each paper first. \"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
